{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data loading and cleaning","metadata":{}},{"cell_type":"markdown","source":"**Import dependancies**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:19.801722Z","iopub.execute_input":"2025-09-24T21:55:19.801921Z","iopub.status.idle":"2025-09-24T21:55:32.889079Z","shell.execute_reply.started":"2025-09-24T21:55:19.801901Z","shell.execute_reply":"2025-09-24T21:55:32.888458Z"}},"outputs":[{"name":"stderr","text":"2025-09-24 21:55:22.100276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758750922.263218      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758750922.317577      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Load data**","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:32.890587Z","iopub.execute_input":"2025-09-24T21:55:32.891060Z","iopub.status.idle":"2025-09-24T21:55:33.332786Z","shell.execute_reply.started":"2025-09-24T21:55:32.891041Z","shell.execute_reply":"2025-09-24T21:55:33.332190Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:33.333401Z","iopub.execute_input":"2025-09-24T21:55:33.333597Z","iopub.status.idle":"2025-09-24T21:55:34.812720Z","shell.execute_reply.started":"2025-09-24T21:55:33.333576Z","shell.execute_reply":"2025-09-24T21:55:34.811863Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Discover and clean data**","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:34.813605Z","iopub.execute_input":"2025-09-24T21:55:34.813874Z","iopub.status.idle":"2025-09-24T21:55:34.819965Z","shell.execute_reply.started":"2025-09-24T21:55:34.813847Z","shell.execute_reply":"2025-09-24T21:55:34.819139Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(50000, 2)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:34.820908Z","iopub.execute_input":"2025-09-24T21:55:34.821158Z","iopub.status.idle":"2025-09-24T21:55:34.849926Z","shell.execute_reply.started":"2025-09-24T21:55:34.821129Z","shell.execute_reply":"2025-09-24T21:55:34.849216Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"data['sentiment'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:34.850881Z","iopub.execute_input":"2025-09-24T21:55:34.851140Z","iopub.status.idle":"2025-09-24T21:55:34.862193Z","shell.execute_reply.started":"2025-09-24T21:55:34.851113Z","shell.execute_reply":"2025-09-24T21:55:34.861450Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"sentiment\npositive    25000\nnegative    25000\nName: count, dtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"The data is balanced","metadata":{}},{"cell_type":"code","source":"data.replace({'sentiment': {'positive': 1, 'negative': 0}}, inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:34.864580Z","iopub.execute_input":"2025-09-24T21:55:34.864800Z","iopub.status.idle":"2025-09-24T21:55:34.894428Z","shell.execute_reply.started":"2025-09-24T21:55:34.864777Z","shell.execute_reply":"2025-09-24T21:55:34.893680Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2075527094.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  data.replace({'sentiment': {'positive': 1, 'negative': 0}}, inplace = True)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:34.895160Z","iopub.execute_input":"2025-09-24T21:55:34.895404Z","iopub.status.idle":"2025-09-24T21:55:34.903286Z","shell.execute_reply.started":"2025-09-24T21:55:34.895380Z","shell.execute_reply":"2025-09-24T21:55:34.902642Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                              review  sentiment\n0  One of the other reviewers has mentioned that ...          1\n1  A wonderful little production. <br /><br />The...          1\n2  I thought this was a wonderful way to spend ti...          1\n3  Basically there's a family where a little boy ...          0\n4  Petter Mattei's \"Love in the Time of Money\" is...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"data['sentiment'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:34.904175Z","iopub.execute_input":"2025-09-24T21:55:34.904412Z","iopub.status.idle":"2025-09-24T21:55:34.917789Z","shell.execute_reply.started":"2025-09-24T21:55:34.904391Z","shell.execute_reply":"2025-09-24T21:55:34.917090Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"sentiment\n1    25000\n0    25000\nName: count, dtype: int64"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"**Split data**","metadata":{}},{"cell_type":"code","source":"train_data, test_data = train_test_split(data, random_state = 42, test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:34.918463Z","iopub.execute_input":"2025-09-24T21:55:34.918682Z","iopub.status.idle":"2025-09-24T21:55:34.941466Z","shell.execute_reply.started":"2025-09-24T21:55:34.918666Z","shell.execute_reply":"2025-09-24T21:55:34.940661Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_data.shape, test_data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:34.942373Z","iopub.execute_input":"2025-09-24T21:55:34.942622Z","iopub.status.idle":"2025-09-24T21:55:34.947296Z","shell.execute_reply.started":"2025-09-24T21:55:34.942597Z","shell.execute_reply":"2025-09-24T21:55:34.946554Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"((40000, 2), (10000, 2))"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"**Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"#Tokenize the data \ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(train_data['review'])\nX_train = pad_sequences(tokenizer.texts_to_sequences(train_data['review']), maxlen=200)\nX_test = pad_sequences(tokenizer.texts_to_sequences(test_data['review']), maxlen=200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:34.947982Z","iopub.execute_input":"2025-09-24T21:55:34.948178Z","iopub.status.idle":"2025-09-24T21:55:44.926794Z","shell.execute_reply.started":"2025-09-24T21:55:34.948158Z","shell.execute_reply":"2025-09-24T21:55:44.926237Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Tokenizer is used to convert words to integers or vectors, in our case we are converting them to integers \n\n**What happens exactly?**\n\nWhen you call fit_on_texts(), the tokenizer scans all the text data and builds a word index (mapping each unique word to an integer, sorted by frequency).\n\nBy setting num_words=5000, you tell it:\n👉 “Keep only the top 5000 most frequent words in the training data. Ignore the rest.”\n\nWords outside this top-5000 list will either:\n\n- be skipped (ignored)\n\n- or replaced by an out-of-vocabulary token (<OOV>) if you set oov_token when creating the tokenizer.\n\n**Why is this useful?**\n\n- Reduce memory usage – text datasets can have tens of thousands of unique words, but many are rare or irrelevant. Limiting vocabulary avoids huge embeddings.\n\n- Prevent overfitting – rare words don’t add much value but increase noise.\n\n- Improve training speed – smaller vocabulary → smaller embedding matrix → faster training.\n\n**Usual choice of num_words**\n\nThere’s no universal fixed number — it depends on your dataset and resources. But here are the common practices:\n\n- Small datasets / simple tasks (e.g. sentiment analysis on IMDB reviews):\nnum_words = 5,000 – 20,000 is often enough.\n(IMDB tutorial in Keras usually uses 10,000).\n\n- Medium datasets (millions of tokens):\nnum_words = 30,000 – 50,000.\n\n- Large datasets (news corpora, Wikipedia, translation):\nnum_words = 100k or more (if GPU/memory allows).\n\n\n**Integers vs. Vectors**\n\n🔹 Integers (tokenizer outputs like [15, 27, 3, 99])\n\n- These are word IDs (indexes in the vocabulary).\n\n- They don’t capture meaning by themselves, but they’re necessary because ML models can’t process raw text.\n\n🔹 Vectors (embeddings like [0.12, -0.45, 0.67, ...])\n\n- These are dense, continuous representations of words.\n\n- They capture semantic meaning (e.g., “king” and “queen” will be close in vector space).\n\n- Usually generated by an Embedding layer in Keras (learned during training or initialized with pretrained embeddings like GloVe/Word2Vec).\n\n**When to use integers?**\n\n- Right after tokenization.\n\n- You keep them as integers to feed into:\n\n    - Embedding layers (Embedding(input_dim=num_words, output_dim=128)) → turns them into vectors automatically.\n\n    - Or classical ML approaches (bag-of-words, TF-IDF) where you don’t need dense embeddings.\n\n**When to use vectors?**\n\n- When feeding text into models like LSTMs, GRUs, Transformers, CNNs, etc.\n\n- You either:\n\n    - Let the Embedding layer learn them (most common).\n\n    - Or load pretrained word vectors if you want semantic knowledge from large corpora.","metadata":{}},{"cell_type":"markdown","source":"- pad_sequences(maxlen=200) → makes all sequences exactly length 200 by :\n    - If a sequence is shorter than 200 → it gets padded with zeros.\n\n    - If a sequence is longer than 200 → it gets truncated (by default from the start).\n\n- Fitting tokenizer only on train data is correct.\n\n    - Test data is transformed with the same word index.\n\n    - Unknown words in test → ignored or mapped to <OOV>.\n \nSo at this stage, your review looks like:\n\n    'This movie was great'\n\n    → [14, 57, 92, 8]\n    \n    → after padding to maxlen=200 → [0, 0, 0, …, 14, 57, 92, 8]\n \nThese are integers, not vectors.","metadata":{}},{"cell_type":"code","source":"X_train, X_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:44.927499Z","iopub.execute_input":"2025-09-24T21:55:44.927698Z","iopub.status.idle":"2025-09-24T21:55:44.933195Z","shell.execute_reply.started":"2025-09-24T21:55:44.927683Z","shell.execute_reply":"2025-09-24T21:55:44.932617Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(array([[1935,    1, 1200, ...,  205,  351, 3856],\n        [   3, 1651,  595, ...,   89,  103,    9],\n        [   0,    0,    0, ...,    2,  710,   62],\n        ...,\n        [   0,    0,    0, ..., 1641,    2,  603],\n        [   0,    0,    0, ...,  245,  103,  125],\n        [   0,    0,    0, ...,   70,   73, 2062]], dtype=int32),\n array([[   0,    0,    0, ...,  995,  719,  155],\n        [  12,  162,   59, ...,  380,    7,    7],\n        [   0,    0,    0, ...,   50, 1088,   96],\n        ...,\n        [   0,    0,    0, ...,  125,  200, 3241],\n        [   0,    0,    0, ..., 1066,    1, 2305],\n        [   0,    0,    0, ...,    1,  332,   27]], dtype=int32))"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"Y_train = train_data['sentiment']\nY_test = test_data['sentiment']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:44.933883Z","iopub.execute_input":"2025-09-24T21:55:44.934601Z","iopub.status.idle":"2025-09-24T21:55:44.947567Z","shell.execute_reply.started":"2025-09-24T21:55:44.934579Z","shell.execute_reply":"2025-09-24T21:55:44.947036Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"Y_train.shape, Y_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:44.948125Z","iopub.execute_input":"2025-09-24T21:55:44.948298Z","iopub.status.idle":"2025-09-24T21:55:44.963933Z","shell.execute_reply.started":"2025-09-24T21:55:44.948284Z","shell.execute_reply":"2025-09-24T21:55:44.963312Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"((40000,), (10000,))"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"Y_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:44.964726Z","iopub.execute_input":"2025-09-24T21:55:44.965197Z","iopub.status.idle":"2025-09-24T21:55:44.980373Z","shell.execute_reply.started":"2025-09-24T21:55:44.965171Z","shell.execute_reply":"2025-09-24T21:55:44.979900Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"39087    0\n30893    0\n45278    1\n16398    0\n13653    0\n        ..\n11284    1\n44732    1\n38158    0\n860      1\n15795    1\nName: sentiment, Length: 40000, dtype: int64"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# LSTM model (Long Short Term Memory)","metadata":{}},{"cell_type":"markdown","source":"**LSTM = Long Short-Term Memory**, a special kind of Recurrent Neural Network (RNN = un type de réseau de neurones artificiels conçu pour traiter des données séquentielles, comme le texte ou les séries temporelles).\n\n- Normal RNNs process sequences word by word and pass information forward, but they forget long-term dependencies (vanishing gradient problem).\n\n- LSTMs solve this by adding a “memory cell” with gates:\n\n    - Forget gate: decide what old info to drop.\n\n    - Input gate: decide what new info to store.\n\n    - Output gate: decide what info to pass to the next step.\n\n👉 This lets them remember important info for a long time (like context in a sentence).\n\n**Advantages over simple feedforward networks**\n\n- Sequence awareness:\nA dense feedforward NN just sees input as a bag of numbers, losing word order.\nLSTM keeps order and context.\n\n- Memory of context:\nThey can “remember” earlier parts of the text when analyzing later parts.\n\n- Better at long sequences:\nUnlike vanilla RNNs, LSTMs handle long sentences without easily forgetting early words.\n\n**So, rule of thumb:**\n\n- If sequence is short/simple and you want a quick baseline → Simple RNN is okay (ex: Predicting the next character in a short word (\"hel → hello\"), Small toy tasks / academic examples (to show how recurrence works), Very small datasets where model simplicity prevents overfitting).\n\n- If sequence is longer or context really matters (which is most NLP problems) → LSTM (or GRU, or Transformers) is better","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n#inpu_dim = max number of words, output_dim = dim of vector, input_length = dim of sentences voctors\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n#Special to recurrent layers (like LSTM/GRU).Instead of dropping inputs, it drops connections in the recurrent state (hidden state between time steps).\n#This helps prevent the model from memorizing sequences too tightly and overfitting.\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:44.981016Z","iopub.execute_input":"2025-09-24T21:55:44.981233Z","iopub.status.idle":"2025-09-24T21:55:45.749222Z","shell.execute_reply.started":"2025-09-24T21:55:44.981210Z","shell.execute_reply":"2025-09-24T21:55:45.748538Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\nI0000 00:00:1758750945.712576      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:45.749966Z","iopub.execute_input":"2025-09-24T21:55:45.750191Z","iopub.status.idle":"2025-09-24T21:55:45.763180Z","shell.execute_reply.started":"2025-09-24T21:55:45.750173Z","shell.execute_reply":"2025-09-24T21:55:45.762540Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"In TensorFlow 2.x / Keras, some layers remain “unbuilt” until the model actually sees input data or you explicitly build it.\nEven though you gave input_length, Keras doesn’t finalize shapes until the model is built.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:45.763913Z","iopub.execute_input":"2025-09-24T21:55:45.764115Z","iopub.status.idle":"2025-09-24T21:55:45.782094Z","shell.execute_reply.started":"2025-09-24T21:55:45.764092Z","shell.execute_reply":"2025-09-24T21:55:45.781600Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"model.fit(X_train, Y_train, epochs=10, batch_size=64, validation_split=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:55:45.782772Z","iopub.execute_input":"2025-09-24T21:55:45.783031Z","iopub.status.idle":"2025-09-24T22:21:19.614003Z","shell.execute_reply.started":"2025-09-24T21:55:45.783011Z","shell.execute_reply":"2025-09-24T22:21:19.613232Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 307ms/step - accuracy: 0.7199 - loss: 0.5433 - val_accuracy: 0.8367 - val_loss: 0.3767\nEpoch 2/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 303ms/step - accuracy: 0.8552 - loss: 0.3528 - val_accuracy: 0.8646 - val_loss: 0.3300\nEpoch 3/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 306ms/step - accuracy: 0.8722 - loss: 0.3115 - val_accuracy: 0.8610 - val_loss: 0.3196\nEpoch 4/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 304ms/step - accuracy: 0.8880 - loss: 0.2783 - val_accuracy: 0.8734 - val_loss: 0.3141\nEpoch 5/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 305ms/step - accuracy: 0.9085 - loss: 0.2372 - val_accuracy: 0.8745 - val_loss: 0.3095\nEpoch 6/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 305ms/step - accuracy: 0.9136 - loss: 0.2184 - val_accuracy: 0.8776 - val_loss: 0.3122\nEpoch 7/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 306ms/step - accuracy: 0.9252 - loss: 0.1938 - val_accuracy: 0.8764 - val_loss: 0.3233\nEpoch 8/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 305ms/step - accuracy: 0.9379 - loss: 0.1644 - val_accuracy: 0.8740 - val_loss: 0.3440\nEpoch 9/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 305ms/step - accuracy: 0.9459 - loss: 0.1470 - val_accuracy: 0.8776 - val_loss: 0.3308\nEpoch 10/10\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 305ms/step - accuracy: 0.9515 - loss: 0.1301 - val_accuracy: 0.8779 - val_loss: 0.3574\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a66b004d050>"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"**Model Evaluation**","metadata":{}},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test, Y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:21:19.614890Z","iopub.execute_input":"2025-09-24T22:21:19.615106Z","iopub.status.idle":"2025-09-24T22:21:50.612905Z","shell.execute_reply.started":"2025-09-24T22:21:19.615090Z","shell.execute_reply":"2025-09-24T22:21:50.612320Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 97ms/step - accuracy: 0.8788 - loss: 0.3330\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**Predictive system**","metadata":{}},{"cell_type":"code","source":"def predict_sentiment(review):\n    sequence = tokenizer.texts_to_sequences([review]) # [] means treat it as a single sentence\n    padded_sequence = pad_sequences(sequence, maxlen=200)\n    prediction = model.predict(padded_sequence)\n    sentiment = 'positive' if prediction[0][0] > 0.5 else 'negative'\n    return sentiment","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:21:50.615739Z","iopub.execute_input":"2025-09-24T22:21:50.615995Z","iopub.status.idle":"2025-09-24T22:21:50.620155Z","shell.execute_reply.started":"2025-09-24T22:21:50.615979Z","shell.execute_reply":"2025-09-24T22:21:50.619447Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"new_review = 'This movie is really good, I loved it'\nsentiment = predict_sentiment(new_review)\nprint(\"The sentiment of the review is : \", sentiment)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:21:50.620876Z","iopub.execute_input":"2025-09-24T22:21:50.621121Z","iopub.status.idle":"2025-09-24T22:21:51.245149Z","shell.execute_reply.started":"2025-09-24T22:21:50.621099Z","shell.execute_reply":"2025-09-24T22:21:51.244574Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563ms/step\nThe sentiment of the review is :  positive\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"new_review = \"The movie didn't catch me, it is basic\"\nsentiment = predict_sentiment(new_review)\nprint(\"The sentiment of the review is : \", sentiment)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:21:51.245925Z","iopub.execute_input":"2025-09-24T22:21:51.246192Z","iopub.status.idle":"2025-09-24T22:21:51.422574Z","shell.execute_reply.started":"2025-09-24T22:21:51.246166Z","shell.execute_reply":"2025-09-24T22:21:51.422008Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\nThe sentiment of the review is :  positive\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"new_review = \"The movie was not that good\"\nsentiment = predict_sentiment(new_review)\nprint(\"The sentiment of the review is : \", sentiment)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:21:51.423287Z","iopub.execute_input":"2025-09-24T22:21:51.423496Z","iopub.status.idle":"2025-09-24T22:21:51.599523Z","shell.execute_reply.started":"2025-09-24T22:21:51.423479Z","shell.execute_reply":"2025-09-24T22:21:51.598955Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\nThe sentiment of the review is :  negative\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"new_review = \"It is a lovely movie\"\nsentiment = predict_sentiment(new_review)\nprint(\"The sentiment of the review is : \", sentiment)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:21:51.600237Z","iopub.execute_input":"2025-09-24T22:21:51.600490Z","iopub.status.idle":"2025-09-24T22:21:51.773601Z","shell.execute_reply.started":"2025-09-24T22:21:51.600471Z","shell.execute_reply":"2025-09-24T22:21:51.773022Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\nThe sentiment of the review is :  positive\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"model.export(\"sentiment_analysis_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:21:51.774288Z","iopub.execute_input":"2025-09-24T22:21:51.774472Z","iopub.status.idle":"2025-09-24T22:21:52.966881Z","shell.execute_reply.started":"2025-09-24T22:21:51.774457Z","shell.execute_reply":"2025-09-24T22:21:52.966299Z"}},"outputs":[{"name":"stdout","text":"Saved artifact at 'sentiment_analysis_model'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 200), dtype=tf.float32, name='keras_tensor')\nOutput Type:\n  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\nCaptures:\n  134579791712080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134579791718608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134579791711504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134581945963664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134579791711888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134581945963472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  134581945961744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import shutil\n\n# Zip the folder\nshutil.make_archive(\"/kaggle/working/sentiment_analysis_model\", 'zip', \"/kaggle/working/sentiment_analysis_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:21:52.967889Z","iopub.execute_input":"2025-09-24T22:21:52.968149Z","iopub.status.idle":"2025-09-24T22:21:53.276345Z","shell.execute_reply.started":"2025-09-24T22:21:52.968123Z","shell.execute_reply":"2025-09-24T22:21:53.275760Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/sentiment_analysis_model.zip'"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"**Saving the tokenizer**","metadata":{}},{"cell_type":"code","source":"import pickle\n\nwith open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:21:53.277029Z","iopub.execute_input":"2025-09-24T22:21:53.277241Z","iopub.status.idle":"2025-09-24T22:21:53.379009Z","shell.execute_reply.started":"2025-09-24T22:21:53.277223Z","shell.execute_reply":"2025-09-24T22:21:53.378419Z"}},"outputs":[],"execution_count":29}]}